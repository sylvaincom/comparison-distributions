{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Empirical estimation of IPMs (integral probability metrics)<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "Author: [Sylvain Combettes](https://github.com/sylvaincom).\n",
    "\n",
    "Last update: Feb 1, 2020.\n",
    "\n",
    "---\n",
    "This notebook deals with the empirical estimation of IPMs (integral probability metrics) and completes my report on the _Comparison of Empirical Probability Distributions_. As $f$-divergences work on probability distributions, IPMs work on samples drawn from the probability distributions. Actually, for IPMs, we only focus on the Kantorovich metric.\n",
    "\n",
    "If needed, see `ipm-prerequisite.ipynb` which introduces the `PuLP` library for solving linear programming problems and gives more details about how the Kantorovich function was built.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>README<span class=\"tocSkip\"></span></h4>\n",
    "<p>\n",
    "The best way to open this Jupyter Notebook is to use the table of contents from the extensions called <code>nbextensions</code>. See <a href=\"https://towardsdatascience.com/4-awesome-tips-for-enhancing-jupyter-notebooks-4d8905f926c5\">4 Awesome Tips for Enhancing Jupyter Notebooks</a> by George Seif.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "In your current directory:\n",
    "<ul>\n",
    "  <li>create a <code>img</code> folder (in which the generated images will be saved)</li>\n",
    "  <li>create another <code>data</code> folder in move the download the <a href=\"https://github.com/sylvaincom/comparison-distributions/tree/master/data\">data from my GitHub</a> into it</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The Python version is 3.7.3.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Defining-a-generic-function-for-computing-the-estimate-of-the-Kantorovich-metric-$W$\" data-toc-modified-id=\"Defining-a-generic-function-for-computing-the-estimate-of-the-Kantorovich-metric-$W$-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Defining a generic function for computing the estimate of the Kantorovich metric $W$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-the-generic-function\" data-toc-modified-id=\"Defining-the-generic-function-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Defining the generic function</a></span></li><li><span><a href=\"#The-memory-error-due-to-the-linear-programming-problem\" data-toc-modified-id=\"The-memory-error-due-to-the-linear-programming-problem-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The memory error due to the linear programming problem</a></span></li></ul></li><li><span><a href=\"#Running-several-simulations-to-interpret-$W$\" data-toc-modified-id=\"Running-several-simulations-to-interpret-$W$-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Running several simulations to interpret $W$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-two-normal-distributions\" data-toc-modified-id=\"Comparing-two-normal-distributions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Comparing two normal distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$W$\" data-toc-modified-id=\"Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$W$-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Influence of the difference of means $\\mu_q-\\mu_p$ on $W$</a></span></li><li><span><a href=\"#Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$W$\" data-toc-modified-id=\"Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$W$-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $W$</a></span></li><li><span><a href=\"#Influence-of-the-number-of-samples-on-$W$\" data-toc-modified-id=\"Influence-of-the-number-of-samples-on-$W$-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Influence of the number of samples on $W$</a></span></li></ul></li><li><span><a href=\"#Comparison-of-two-exponential-distributions\" data-toc-modified-id=\"Comparison-of-two-exponential-distributions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Comparison of two exponential distributions</a></span></li><li><span><a href=\"#Comparison-of-two-uniform-distributions\" data-toc-modified-id=\"Comparison-of-two-uniform-distributions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Comparison of two uniform distributions</a></span></li></ul></li><li><span><a href=\"#Application-of-the-Kantorovich-metric-to-the-data-generated-from-two-methods-for-computing-the-Choquet-integral\" data-toc-modified-id=\"Application-of-the-Kantorovich-metric-to-the-data-generated-from-two-methods-for-computing-the-Choquet-integral-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Application of the Kantorovich metric to the data generated from two methods for computing the Choquet integral</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-the-Choquet-integral-of-normal-distributions\" data-toc-modified-id=\"For-the-Choquet-integral-of-normal-distributions-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>For the Choquet integral of normal distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Presenting-the-data\" data-toc-modified-id=\"Presenting-the-data-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Presenting the data</a></span></li><li><span><a href=\"#Computing-the-Kantorovich-metric\" data-toc-modified-id=\"Computing-the-Kantorovich-metric-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Computing the Kantorovich metric</a></span></li></ul></li><li><span><a href=\"#For-the-Choquet-integral-of-exponential-distributions\" data-toc-modified-id=\"For-the-Choquet-integral-of-exponential-distributions-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>For the Choquet integral of exponential distributions</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports<span class=\"tocSkip\"></span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pulp\n",
    "from pulp import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from time import process_time\n",
    "import datetime\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the size of the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a generic function for computing the estimate of the Kantorovich metric $W$\n",
    "\n",
    "## Defining the generic function\n",
    "\n",
    "We duplicate the functions created in `ipm-prerequisite.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_c(X_p, X_q):\n",
    "    m = len(X_p)\n",
    "    n = len(X_q)\n",
    "    \n",
    "    Y = [1/m]*m + [-1/n]*n\n",
    "    \n",
    "    return np.asarray(Y)\n",
    "\n",
    "\n",
    "def rho(x,y):\n",
    "    return abs(x-y)\n",
    "\n",
    "\n",
    "def construct_b(X_p, X_q):\n",
    "    \n",
    "    X = np.concatenate((X_p, X_q), axis=0)\n",
    "    N = len(X)\n",
    "    \n",
    "    b_part = []\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            b_part.append(rho(X[i], X[j]))\n",
    "    \n",
    "    # Now, we duplicate each row to obtain a list of size 2*N\n",
    "    b = []\n",
    "    for i in range(N):\n",
    "        b.append(b_part[i])\n",
    "        b.append(b_part[i])\n",
    "    \n",
    "    return b\n",
    "\n",
    "\n",
    "def construct_M(X_p, X_q):\n",
    "    \n",
    "    X = np.concatenate((X_p, X_q), axis=0)\n",
    "    N = len(X)\n",
    "    \n",
    "    M = []\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            l_M_1 = [0]*N\n",
    "            l_M_1[i] = 1\n",
    "            l_M_1[j] = -1\n",
    "            M.append(l_M_1)\n",
    "            l_M_2 = [0]*N\n",
    "            l_M_2[i] = -1\n",
    "            l_M_2[j] = 1\n",
    "            M.append(l_M_2)\n",
    "    M = np.asarray(M)\n",
    "\n",
    "    return M.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we regroup them under one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kantorovich_metric(X_p, X_q):\n",
    "    \n",
    "    X = np.concatenate((X_p, X_q), axis=0)\n",
    "    m = len(X_p)\n",
    "    n = len(X_q)\n",
    "    N = m+n\n",
    "    \n",
    "    c = construct_c(X_p, X_q)\n",
    "    b = construct_b(X_p, X_q)\n",
    "    M = construct_M(X_p, X_q)\n",
    "    \n",
    "    prob = LpProblem(\"LP problem for estimating the Kantorovich metric\", LpMaximize)\n",
    "    a = LpVariable.matrix(\"a\", list(range(N)))\n",
    "    prob += lpDot(c, a)\n",
    "    p = 2*N\n",
    "    for i in range(p):\n",
    "        prob += lpDot(M[i], a) <= b[i]\n",
    "    prob.solve()\n",
    "    \n",
    "#     for v in prob.variables():\n",
    "#         print(v.name, \"=\", v.varValue)\n",
    "        \n",
    "#     print(\"objective=\", value(prob.objective))\n",
    "\n",
    "    return value(prob.objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check if we obtain the same result as formula (II.11) of the report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.uniform(1, 2, 10)\n",
    "np.random.seed(2) # random seed for reproducability\n",
    "X_q = np.random.uniform(9, 10, 10)\n",
    "W = kantorovich_metric(X_p, X_q)\n",
    "print('The empirical Kantorovich metric is: \\n', round(W, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is close to $8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The memory error due to the linear programming problem\n",
    "\n",
    "We take the same parameters as previsouly but with more samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_start = process_time()\n",
    "# np.random.seed(1) # random seed for reproducability\n",
    "# X_p = np.random.uniform(1, 2, 100)\n",
    "# np.random.seed(2) # random seed for reproducability\n",
    "# X_q = np.random.uniform(9, 10, 100)\n",
    "# kantorovich_metric(X_p, X_q)\n",
    "# t_stop = process_time()\n",
    "# print('The empirical Kantorovich metric is: \\n', W)\n",
    "# print('The processing time is: \\n', datetime.timedelta(seconds=t_stop-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_start = process_time()\n",
    "# np.random.seed(1) # random seed for reproducability\n",
    "# X_p = np.random.uniform(1, 2, 200)\n",
    "# np.random.seed(2) # random seed for reproducability\n",
    "# X_q = np.random.uniform(9, 10, 200)\n",
    "# W = kantorovich_metric(X_p, X_q)\n",
    "# t_stop = process_time()\n",
    "# print('The empirical Kantorovich metric is: \\n', W)\n",
    "# print('The processing time is: \\n', datetime.timedelta(seconds=t_stop-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_start = process_time()\n",
    "# np.random.seed(1) # random seed for reproducability\n",
    "# X_p = np.random.uniform(1, 2, 300)\n",
    "# np.random.seed(2) # random seed for reproducability\n",
    "# X_q = np.random.uniform(9, 10, 300)\n",
    "# W = kantorovich_metric(X_p, X_q)\n",
    "# t_stop = process_time()\n",
    "# print('The empirical Kantorovich metric is: \\n', W)\n",
    "# print('The processing time is: \\n', datetime.timedelta(seconds=t_stop-t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a memory error for $300$ samples per distribution !\n",
    "\n",
    "The problem is that when we have more than $400$ total samples (for the two distributions), $M$ already has $400 \\times 399 \\times 400 = 63~840~000$ values and we have a memory problem if we go above $400$ total samples. Hence, we can not have more than $200$ samples per distribution.\n",
    "\n",
    "Note that the problem is not really about the computing time (which of course, we could have tried to improve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running several simulations to interpret $W$\n",
    "\n",
    "Check how it evolves with the number of samples, compute the empirical standard deviation and mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two normal distributions\n",
    "\n",
    "Here we consider two normal distributions $\\mathbb{P} = \\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathbb{Q} = \\mathcal{N}(\\mu_q, \\sigma_q)$.\n",
    "\n",
    "### Influence of the difference of means $\\mu_q-\\mu_p$ on $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_p, sigma_p, n_p, mu_q, sigma_q, n_q = 0, 2, 50, 1, 2, 50\n",
    "np.random.seed(2) # random seed for reproducability\n",
    "X_p = np.random.normal(mu_p, sigma_p, n_p)\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_q = np.random.normal(mu_q, sigma_q, n_q)\n",
    "\n",
    "txt = 'Histograms of $X_p$ and $X_q$ from normal distributions '\n",
    "txt = txt + '$\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'The Kantorovich metric $W(X_p, X_q)$ is %1.3f' % kantorovich_metric(X_p, X_q)\n",
    "plt.title(txt)\n",
    "sns.distplot(X_p) ;\n",
    "sns.distplot(X_q) ;\n",
    "txt1 = '$\\mu_p = %1.1f$, $\\sigma_p = %1.0f$, $n_p= %1.0f$' % (mu_p, sigma_p, n_p)\n",
    "txt2 = '$\\mu_q = %1.1f$, $\\sigma_q = %1.0f$, $n_q= %1.0f$' % (mu_q, sigma_q, n_q)\n",
    "plt.legend([txt1, txt2]) ;\n",
    "plt.savefig('img/IPM_normal_histo_1.png', dpi=120) ; # to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_p, sigma_p, n_p, mu_q, sigma_q, n_q = 0, 2, 50, 5, 2, 50\n",
    "np.random.seed(2) # random seed for reproducability\n",
    "X_p = np.random.normal(mu_p, sigma_p, n_p)\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_q = np.random.normal(mu_q, sigma_q, n_q)\n",
    "\n",
    "txt = 'Histograms of $X_p$ and $X_q$ from normal distributions '\n",
    "txt = txt + '$\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'The Kantorovich metric $W(X_p, X_q)$ is %1.3f' % kantorovich_metric(X_p, X_q)\n",
    "plt.title(txt)\n",
    "sns.distplot(X_p) ;\n",
    "sns.distplot(X_q) ;\n",
    "txt1 = '$\\mu_p = %1.1f$, $\\sigma_p = %1.0f$, $n_p= %1.0f$' % (mu_p, sigma_p, n_p)\n",
    "txt2 = '$\\mu_q = %1.1f$, $\\sigma_q = %1.0f$, $n_q= %1.0f$' % (mu_q, sigma_q, n_q)\n",
    "plt.legend([txt1, txt2]) ;\n",
    "plt.savefig('img/IPM_normal_histo_2.png', dpi=120) ; # to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IPM_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "mu_p = -5\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.normal(mu_p, sigma_p, n_p)\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20+0.5, 0.5)\n",
    "for mu_q in l_abs:\n",
    "    np.random.seed(int(mu_q*10)) # random seed for reproducability, different each time\n",
    "    X_q = np.random.normal(mu_q, sigma_q, n_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_IPM_mu.append([kantorovich_metric(X_p, X_q)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $n_p = %1.0f$, $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (n_p, mu_p, sigma_p)\n",
    "txt = txt + '$n_q = %1.0f$, $\\sigma_q = %1.0f$' % (n_q, sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical Kantorovich metric $W(p, q)$')\n",
    "plt.plot(l_diff_mu, l_IPM_mu, 'o') ;\n",
    "plt.savefig('img/IPM_normal_diff_mu.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we say that the dependency of $W(p,q)$ to $\\mu_q-\\mu_p$ is linear? Yes. Indeed, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_mu), 1)\n",
    "y = np.asarray(l_IPM_mu)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', round(reg.score(X, y), 3))\n",
    "print('The regression coefficients are: \\n', np.round(reg.coef_, 3))\n",
    "print('The regression intercept is: \\n', np.round(reg.intercept_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the the linear regression function from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and the [score method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) returns the coefficient of determination `R^2` of the prediction. The coefficient `R^2` is defined as `(1 - u/v)`, where $u$ is the residual sum of squares `((y_true - y_pred) ** 2).sum()` and `v` is the total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best possible score is $1.0$ and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a `R^2` score of `0.0`.\n",
    "\n",
    "Now we do the same as previously, but by changing the fixed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IPM_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "mu_p = 3\n",
    "sigma_p = 4\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.normal(mu_p, sigma_p, n_p)\n",
    "\n",
    "sigma_q = 4\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20+0.5, 0.5)\n",
    "for mu_q in l_abs:\n",
    "    np.random.seed(int(mu_q*10)) # random seed for reproducability, different each time\n",
    "    X_q = np.random.normal(mu_q, sigma_q, n_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_IPM_mu.append([kantorovich_metric(X_p, X_q)])\n",
    "\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $n_p = %1.0f$, $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (n_p, mu_p, sigma_p)\n",
    "txt = txt + '$n_q = %1.0f$, $\\sigma_q = %1.0f$' % (n_q, sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical Kantorovich metric $W(p, q)$')\n",
    "plt.plot(l_diff_mu, l_IPM_mu, 'o') ;\n",
    "plt.savefig('img/IPM_normal_diff_mu_2.png', dpi=120) # to save the figure\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_mu), 2)\n",
    "y = np.asarray(l_IPM_mu)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', round(reg.score(X, y), 3))\n",
    "print('The regression coefficients are: \\n', np.round(reg.coef_, 3))\n",
    "print('The regression intercept is: \\n', np.round(reg.intercept_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IPM_sigma = []\n",
    "l_diff_sigma = []\n",
    "\n",
    "mu_p = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.normal(mu_p, sigma_p, n_p)\n",
    "\n",
    "mu_q = 0\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 10, 0.25)\n",
    "for sigma_q in l_abs:\n",
    "    np.random.seed(int(sigma_q*10)) # random seed for reproducability, different each time\n",
    "    X_q = np.random.normal(mu_q, sigma_q, n_q)\n",
    "    l_diff_sigma.append([sigma_q-sigma_p])\n",
    "    l_IPM_sigma.append([kantorovich_metric(X_p, X_q)])\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $n_p = %1.0f$, $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (n_p, mu_p, sigma_p)\n",
    "txt = txt + '$n_q = %1.0f$, $\\mu_q = %1.0f$' % (n_q, mu_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of standard deviations $\\sigma_q-\\sigma_p$')\n",
    "plt.ylabel('Empirical Kantorovich metric $W(p, q)$')\n",
    "plt.plot(l_diff_sigma, l_IPM_sigma, 'o') ;\n",
    "plt.savefig('img/IPM_normal_diff_sigma.png', dpi=120) # to save the figure\n",
    "\n",
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_sigma), 1)\n",
    "y = np.asarray(l_IPM_sigma)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', round(reg.score(X, y), 3))\n",
    "print('The regression coefficients are: \\n', np.round(reg.coef_, 3))\n",
    "print('The regression intercept is: \\n', np.round(reg.intercept_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of the number of samples on $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IPM_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "mu_p = -5\n",
    "sigma_p = 2\n",
    "n_p = 10 # after several simulations, 30 seems enough\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.normal(mu_p, sigma_p, n_p)\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20+0.5, 0.5)\n",
    "for mu_q in l_abs:\n",
    "    np.random.seed(int(mu_q*10)) # random seed for reproducability, different each time\n",
    "    X_q = np.random.normal(mu_q, sigma_q, n_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_IPM_mu.append([kantorovich_metric(X_p, X_q)])\n",
    "\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $n_p = %1.0f$, $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (n_p, mu_p, sigma_p)\n",
    "txt = txt + '$n_q = %1.0f$, $\\sigma_q = %1.0f$' % (n_q, sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical Kantorovich metric $W(p, q)$')\n",
    "plt.plot(l_diff_mu, l_IPM_mu, 'o') ;\n",
    "plt.savefig('img/IPM_normal_diff_n.png', dpi=120) # to save the figure\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_mu), 1)\n",
    "y = np.asarray(l_IPM_mu)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', round(reg.score(X, y), 3))\n",
    "print('The regression coefficients are: \\n', np.round(reg.coef_, 3))\n",
    "print('The regression intercept is: \\n', np.round(reg.intercept_, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IPM_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "mu_p = -5\n",
    "sigma_p = 2\n",
    "n_p = 50 # after several simulations, 30 seems enough\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.normal(mu_p, sigma_p, n_p)\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20+0.5, 0.5)\n",
    "for mu_q in l_abs:\n",
    "    np.random.seed(int(mu_q*10)) # random seed for reproducability, different each time\n",
    "    X_q = np.random.normal(mu_q, sigma_q, n_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_IPM_mu.append([kantorovich_metric(X_p, X_q)])\n",
    "\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $n_p = %1.0f$, $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (n_p, mu_p, sigma_p)\n",
    "txt = txt + '$n_q = %1.0f$, $\\sigma_q = %1.0f$' % (n_q, sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical Kantorovich metric $W(p, q)$')\n",
    "plt.plot(l_diff_mu, l_IPM_mu, 'o') ;\n",
    "plt.savefig('img/IPM_normal_diff_n_2.png', dpi=120) # to save the figure\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_mu), 1)\n",
    "y = np.asarray(l_IPM_mu)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', round(reg.score(X, y), 3))\n",
    "print('The regression coefficients are: \\n', np.round(reg.coef_, 3))\n",
    "print('The regression intercept is: \\n', np.round(reg.intercept_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of two exponential distributions\n",
    "\n",
    "\n",
    "Here we consider two exponential distributions $\\mathbb{P} = \\mathcal{E}(\\lambda_p)$ and $\\mathbb{Q} = \\mathcal{E}(\\lambda_q)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IPM_lambda = []\n",
    "l_diff_lambda = []\n",
    "\n",
    "lambda_p = 1\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.exponential(lambda_p, n_p)\n",
    "\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(1, 20, 0.5)\n",
    "for lambda_q in l_abs:\n",
    "    np.random.seed(int(lambda_q*10)) # random seed for reproducability, different each time\n",
    "    X_q = np.random.exponential(lambda_q, n_q)\n",
    "    l_diff_lambda.append([lambda_q-lambda_p])\n",
    "    l_IPM_lambda.append([kantorovich_metric(X_p, X_q)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Comparison of two exponential distributions $\\mathcal{E}(\\lambda_p)$ and $\\mathcal{E}(\\lambda_q)$ \\n'\n",
    "txt = txt + 'with $n_p = %1.0f$, $\\lambda_p = %1.0f$, ' % (n_p, lambda_p)\n",
    "txt = txt + '$n_q = %1.0f$' % (n_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of parameters $\\lambda_q-\\lambda_p$')\n",
    "plt.ylabel('Empirical Kantorovich metric $W(p, q)$')\n",
    "plt.plot(l_diff_lambda, l_IPM_lambda, 'o') ;\n",
    "plt.savefig('img/IPM_exponential_diff.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_lambda), 1)\n",
    "y = np.asarray(l_IPM_lambda)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', round(reg.score(X, y), 3))\n",
    "print('The regression coefficients are: \\n', np.round(reg.coef_, 3))\n",
    "print('The regression intercept is: \\n', np.round(reg.intercept_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of two uniform distributions\n",
    "\n",
    "Here we consider two normal distributions $\\mathbb{P} = \\mathbb{U}\\left([a, a+h]\\right)$ and $\\mathbb{Q} = \\mathbb{U}\\left([r, r+h]\\right)$ where $h$ is the length of the intervals.\n",
    "\n",
    "Note: $a$ is called the **interval start** of $[a, a+h]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IPM_unif = []\n",
    "l_diff_unif = []\n",
    "\n",
    "h = 2 # length of the intervals\n",
    "a = 0\n",
    "b = a+h\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1) # random seed for reproducability\n",
    "X_p = np.random.uniform(a, b, n_p)\n",
    "\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20, 0.5)\n",
    "for r in l_abs:\n",
    "    np.random.seed(int(r*10)) # random seed for reproducability, different each time\n",
    "    X_q = np.random.uniform(r, r+h, n_q)\n",
    "    l_diff_unif.append([r-a])\n",
    "    l_IPM_unif.append([kantorovich_metric(X_p, X_q)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Comparison of two uniform distributions $\\mathbb{U}([a, a+h])$ and $\\mathbb{U}([r, r+h])$ \\n'\n",
    "txt = txt + 'with $n_p = %1.0f$, $a = %1.0f$, $h = %1.0f, $' % (n_p, a, h)\n",
    "txt = txt + '$n_q = %1.0f$, $h = %1.0f$' % (n_q, h)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of interval starts $r-a$')\n",
    "plt.ylabel('Empirical Kantorovich metric $W(p, q)$')\n",
    "plt.plot(l_diff_unif, l_IPM_unif, 'o') ;\n",
    "plt.savefig('img/IPM_uniform_diff.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_unif), 1)\n",
    "y = np.asarray(l_IPM_unif)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', round(reg.score(X, y), 3))\n",
    "print('The regression coefficients are: \\n', np.round(reg.coef_, 3))\n",
    "print('The regression intercept is: \\n', np.round(reg.intercept_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of the Kantorovich metric to the data generated from two methods for computing the Choquet integral\n",
    "\n",
    "## For the Choquet integral of normal distributions\n",
    "\n",
    "### Presenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_df = pd.read_csv('data/X1_normal.csv', sep=',', header=None)\n",
    "Xq_df = pd.read_csv('data/X2_normal.csv', sep=',', header=None)\n",
    "\n",
    "Xp_list = Xp_df.values.tolist()\n",
    "Xq_list = Xq_df.values.tolist()\n",
    "\n",
    "sns.distplot(Xp_list) ;\n",
    "sns.distplot(Xq_list) ;\n",
    "plt.title('Histogram of the samples from the Choquet integral \\n of two normal distributions') ;\n",
    "plt.legend(['with the direct method', 'with the new formula']) ;\n",
    "plt.savefig('img/choquet_normal_X.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are `Xp` and `Xq` from normal distributions? We use the `scipy.stats.normaltest` function [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html) which tests the null hypothesis that a sample comes from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3\n",
    "\n",
    "k2, p = stats.normaltest(Xp_list)\n",
    "print('The pvalue is: \\n', p)\n",
    "\n",
    "print('Null hypothesis: Xp comes from a normal distribution: Xp does not come from a normal distribution.')\n",
    "if p < alpha:  \n",
    "    print(\"The null hypothesis can be rejected.\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3\n",
    "\n",
    "k2, p = stats.normaltest(Xq_list)\n",
    "print('The pvalue is: \\n', p)\n",
    "\n",
    "print('Null hypothesis: Xq comes from a normal distribution.')\n",
    "if p < alpha:  \n",
    "    print(\"The null hypothesis can be rejected: q does not come from a normal distribution.\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Kantorovich metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = process_time()\n",
    "l_IPM_choquet_normal = []\n",
    "\n",
    "random.seed(1)\n",
    "Xp = random.sample(Xp_list, 100)\n",
    "\n",
    "for i in range(10, 20):\n",
    "    random.seed(i)\n",
    "    Xq = random.sample(Xq_list, 100)\n",
    "    W = kantorovich_metric(Xp, Xq)\n",
    "    l_IPM_choquet_normal.append(W)\n",
    "t_stop = process_time()\n",
    "\n",
    "print('The mean of the Kantorovich metrics is: \\n', np.round(np.mean(l_IPM_choquet_normal), 3))\n",
    "print('The standard deviation of the Kantorovich metrics is: \\n', np.round(np.std(l_IPM_choquet_normal), 3))\n",
    "print('The total processing time is: \\n', datetime.timedelta(seconds=t_stop-t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the Choquet integral of exponential distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_df = pd.read_csv('data/X1_exp.csv', sep=',', header=None)\n",
    "Xq_df = pd.read_csv('data/X2_exp.csv', sep=',', header=None)\n",
    "\n",
    "Xp_list = Xp_df.values.tolist()\n",
    "Xq_list = Xq_df.values.tolist()\n",
    "\n",
    "sns.distplot(Xp_list) ;\n",
    "sns.distplot(Xq_list) ;\n",
    "plt.title('Histogram of the samples from the Choquet integral \\n of two exponential distributions') ;\n",
    "plt.legend(['with the direct method', 'with the new formula']) ;\n",
    "plt.savefig('img/choquet_exp_X.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = process_time()\n",
    "l_IPM_choquet_exp = []\n",
    "\n",
    "random.seed(1)\n",
    "Xp = random.sample(Xp_list, 100)\n",
    "\n",
    "for i in range(10, 20):\n",
    "    random.seed(i)\n",
    "    Xq = random.sample(Xq_list, 100)\n",
    "    W = kantorovich_metric(Xp, Xq)\n",
    "    l_IPM_choquet_exp.append(W)\n",
    "t_stop = process_time()\n",
    "\n",
    "print('The mean of the Kantorovich metrics is: \\n', np.round(np.mean(l_IPM_choquet_exp), 3))\n",
    "print('The standard deviation of the Kantorovich metrics is: \\n', np.round(np.std(l_IPM_choquet_exp), 3))\n",
    "print('The total processing time is: \\n', datetime.timedelta(seconds=t_stop-t_start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
