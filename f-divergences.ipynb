{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Empirical estimation of $f$-divergences<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "Author: [Sylvain Combettes](https://github.com/sylvaincom).\n",
    "\n",
    "Last update: Jan 29, 2020.\n",
    "\n",
    "---\n",
    "This notebook deals with the empirical estimation of $f$-divergences and completes my report on the _Comparison of empirical probability distributions_. Three $f$-divergences are dealt with: Kullback-Leibler divergence, Helligence distance and Variational distance. As IPMs (integral probability metrics) work on samples drawn from the probability distributions, ùëì-divergences work on probability distributions.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>README<span class=\"tocSkip\"></span></h4><p>\n",
    "The best way to open this Jupyter Notebook is to use the table of contents from the extensions called <code>nbextensions</code>. See <a href=\"https://towardsdatascience.com/4-awesome-tips-for-enhancing-jupyter-notebooks-4d8905f926c5\">4 Awesome Tips for Enhancing Jupyter Notebooks</a> by George Seif.\n",
    "    \n",
    "The Python version is 3.7.3.\n",
    "</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#KL-divergence\" data-toc-modified-id=\"KL-divergence-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>KL divergence</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-our-generic-function\" data-toc-modified-id=\"Defining-our-generic-function-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Defining our generic function</a></span></li><li><span><a href=\"#Running-several-simulations-to-interpret-$D_{\\text{KL}}$\" data-toc-modified-id=\"Running-several-simulations-to-interpret-$D_{\\text{KL}}$-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Running several simulations to interpret $D_{\\text{KL}}$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-two-normal-distributions\" data-toc-modified-id=\"Comparing-two-normal-distributions-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Comparing two normal distributions</a></span></li></ul></li></ul></li><li><span><a href=\"#Hellinger-distance\" data-toc-modified-id=\"Hellinger-distance-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Hellinger distance</a></span></li><li><span><a href=\"#Variational-distance\" data-toc-modified-id=\"Variational-distance-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Variational distance</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports<span class=\"tocSkip\"></span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the size of the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence\n",
    "\n",
    "This section is inspired from [KL Divergence Python Example](https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our generic function\n",
    "\n",
    "We define our `kl_divergence` function using functions from `numpy`. We are careful with the result $\\lim\\limits_{x \\rightarrow 0} \\log(x) = -\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Kullback-Leibler divergence of two (empirical) probability distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : numpy.ndarray\n",
    "        Vector of the values of the first (discrete) probability distribution.\n",
    "    q : numpy.ndarray\n",
    "        Vector of the values of the second (discrete) probability distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : numpy.float64\n",
    "        Result of the computation of the Kullback-Leibler divergence of p from q.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = np.sum(np.where(p!=0, p*np.log(p/q), 0))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running several simulations to interpret $D_{\\text{KL}}$\n",
    "\n",
    "### Comparing two normal distributions\n",
    "\n",
    "Now, we are going to plot two normal distributions using [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) and compute their KL divergence, for several values of mean and standard deviation. The goal is to see if the interpretation of the KL divergence corresponds to our intuition.\n",
    "\n",
    "We define a plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal_divergence(m_p, sd_p, m_q, sd_q, f_divergence):\n",
    "    \"\"\"\n",
    "    Plotting two normal distributions and computing their f-divergence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m_p : float\n",
    "        Mean of the normal distribution p.\n",
    "    sd_p : float\n",
    "        Standard deviation of the normal distribution p.\n",
    "    m_q : float\n",
    "        Mean of the normal distribution p.\n",
    "    sd_q : float\n",
    "        Standard deviation distribution p.\n",
    "    f_divergence : {kl_divergence, hellinger_distance, varational_distance}\n",
    "        Function that computes the f-divergence we choose.\n",
    "        \n",
    "    Plots\n",
    "    -------\n",
    "    Plots (on the same figure) the two normal distributions and their f-divergence in the title.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "    p = norm.pdf(x, m_p, sd_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "    q = norm.pdf(x, m_q, sd_q) # second normal distribution of mean m_q and standard deviation sd_q\n",
    "    \n",
    "    plt.title('The %s of $p$ from $q$ is %1.3f \\n (with $p$ and $q$ normal distributions)'\n",
    "              % (f_divergence.__name__, f_divergence(p, q)))\n",
    "    plt.plot(x, p)\n",
    "    plt.plot(x, q, c='red')\n",
    "    txt1 = '$m_p = %1.1f$ and $sd_p = %1.0f$' % (m_p, sd_p)\n",
    "    txt2 = '$m_q = %1.1f$ and $sd_q = %1.0f$' % (m_q, sd_q)\n",
    "    plt.legend([txt1, txt2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_divergence(0, 2, 1, 2, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 2, 2, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 4, 2, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 0, 1, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 0, 3, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 0, 4, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 4, 1, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 4, 2, kl_divergence)\n",
    "plot_normal_divergence(0, 2, 4, 4, kl_divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It‚Äôs important to note that the KL divergence is not symmetrical. In other words, if we switch `p` for `q` and vice versa, we get a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_divergence(5, 4, 0, 2, kl_divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the KL divergence, the closer the two distributions are to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hellinger distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger_distance(p, q):\n",
    "    return np.sum((np.sqrt(p)-np.sqrt(q))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence(0, 2, 1, 2, hellinger_distance)\n",
    "plot_divergence(0, 2, 2, 2, hellinger_distance)\n",
    "plot_divergence(0, 2, 4, 2, hellinger_distance)\n",
    "plot_divergence(0, 2, 0, 1, hellinger_distance)\n",
    "plot_divergence(0, 2, 0, 3, hellinger_distance)\n",
    "plot_divergence(0, 2, 0, 4, hellinger_distance)\n",
    "plot_divergence(0, 2, 4, 1, hellinger_distance)\n",
    "plot_divergence(0, 2, 4, 2, hellinger_distance)\n",
    "plot_divergence(0, 2, 4, 4, hellinger_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_distance(p, q):\n",
    "    return np.sum(np.abs(p-q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_divergence(0, 2, 1, 2, variational_distance)\n",
    "plot_divergence(0, 2, 2, 2, variational_distance)\n",
    "plot_divergence(0, 2, 4, 2, variational_distance)\n",
    "plot_divergence(0, 2, 0, 1, variational_distance)\n",
    "plot_divergence(0, 2, 0, 3, variational_distance)\n",
    "plot_divergence(0, 2, 0, 4, variational_distance)\n",
    "plot_divergence(0, 2, 4, 1, variational_distance)\n",
    "plot_divergence(0, 2, 4, 2, variational_distance)\n",
    "plot_divergence(0, 2, 4, 4, variational_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
