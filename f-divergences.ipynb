{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Empirical estimation of $f$-divergences<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "Author: [Sylvain Combettes](https://github.com/sylvaincom).\n",
    "\n",
    "Last update: Feb 2, 2020.\n",
    "\n",
    "---\n",
    "This notebook deals with the empirical estimation of $f$-divergences and completes my report on the _Comparison of Empirical Probability Distributions_. Three $f$-divergences are dealt with: Kullback-Leibler divergence $D_{KL}$, Helligence distance $D_H$ and Variational distance $D_V$. Contrary to IPMs (integral probability metrics) that work on samples drawn from the probability distributions, $f$-divergences work on probability distributions.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>README<span class=\"tocSkip\"></span></h4><p>\n",
    "The best way to open this Jupyter Notebook is to use the table of contents from the extensions called <code>nbextensions</code>. See <a href=\"https://towardsdatascience.com/4-awesome-tips-for-enhancing-jupyter-notebooks-4d8905f926c5\">4 Awesome Tips for Enhancing Jupyter Notebooks</a> by George Seif.\n",
    "    \n",
    "The Python version is 3.7.3.\n",
    "</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#KL-divergence\" data-toc-modified-id=\"KL-divergence-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>KL divergence</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-our-generic-function\" data-toc-modified-id=\"Defining-our-generic-function-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Defining our generic function</a></span></li><li><span><a href=\"#Running-several-simulations-to-interpret-$D_{\\text{KL}}$\" data-toc-modified-id=\"Running-several-simulations-to-interpret-$D_{\\text{KL}}$-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Running several simulations to interpret $D_{\\text{KL}}$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-two-normal-distributions\" data-toc-modified-id=\"Comparing-two-normal-distributions-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Comparing two normal distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$D_\\text{KL}$\" data-toc-modified-id=\"Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$D_\\text{KL}$-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>Influence of the difference of means $\\mu_q-\\mu_p$ on $D_\\text{KL}$</a></span></li><li><span><a href=\"#Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$D_\\text{KL}$\" data-toc-modified-id=\"Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$D_\\text{KL}$-1.2.1.2\"><span class=\"toc-item-num\">1.2.1.2&nbsp;&nbsp;</span>Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $D_\\text{KL}$</a></span></li></ul></li><li><span><a href=\"#Comparison-of-two-exponential-distributions\" data-toc-modified-id=\"Comparison-of-two-exponential-distributions-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Comparison of two exponential distributions</a></span></li><li><span><a href=\"#Comparison-of-two-uniform-distributions\" data-toc-modified-id=\"Comparison-of-two-uniform-distributions-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Comparison of two uniform distributions</a></span></li></ul></li></ul></li><li><span><a href=\"#Hellinger-distance\" data-toc-modified-id=\"Hellinger-distance-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Hellinger distance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-our-generic-function\" data-toc-modified-id=\"Defining-our-generic-function-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Defining our generic function</a></span></li><li><span><a href=\"#Running-several-simulations-to-interpret-$D_H$\" data-toc-modified-id=\"Running-several-simulations-to-interpret-$D_H$-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Running several simulations to interpret $D_H$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-two-normal-distributions\" data-toc-modified-id=\"Comparing-two-normal-distributions-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Comparing two normal distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$D_H$\" data-toc-modified-id=\"Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$D_H$-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Influence of the difference of means $\\mu_q-\\mu_p$ on $D_H$</a></span></li><li><span><a href=\"#Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$D_H$\" data-toc-modified-id=\"Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$D_H$-2.2.1.2\"><span class=\"toc-item-num\">2.2.1.2&nbsp;&nbsp;</span>Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $D_H$</a></span></li></ul></li><li><span><a href=\"#Comparing-two-exponential-distributions\" data-toc-modified-id=\"Comparing-two-exponential-distributions-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Comparing two exponential distributions</a></span></li><li><span><a href=\"#Comparing-two-uniform-distributions\" data-toc-modified-id=\"Comparing-two-uniform-distributions-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Comparing two uniform distributions</a></span></li></ul></li></ul></li><li><span><a href=\"#Variational-distance\" data-toc-modified-id=\"Variational-distance-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Variational distance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-our-generic-function\" data-toc-modified-id=\"Defining-our-generic-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Defining our generic function</a></span></li><li><span><a href=\"#Running-several-simulations-to-interpret-$D_V$\" data-toc-modified-id=\"Running-several-simulations-to-interpret-$D_V$-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Running several simulations to interpret $D_V$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-two-normal-distributions\" data-toc-modified-id=\"Comparing-two-normal-distributions-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Comparing two normal distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$D_V$\" data-toc-modified-id=\"Influence-of-the-difference-of-means-$\\mu_q-\\mu_p$-on-$D_V$-3.2.1.1\"><span class=\"toc-item-num\">3.2.1.1&nbsp;&nbsp;</span>Influence of the difference of means $\\mu_q-\\mu_p$ on $D_V$</a></span></li><li><span><a href=\"#Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$D_V$\" data-toc-modified-id=\"Influence-of-the-difference-of-standard-deviations-$\\sigma_q-\\sigma_p$-on-$D_V$-3.2.1.2\"><span class=\"toc-item-num\">3.2.1.2&nbsp;&nbsp;</span>Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $D_V$</a></span></li></ul></li><li><span><a href=\"#Comparing-two-exponential-distributions\" data-toc-modified-id=\"Comparing-two-exponential-distributions-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Comparing two exponential distributions</a></span></li><li><span><a href=\"#Comparing-two-uniform-distributions\" data-toc-modified-id=\"Comparing-two-uniform-distributions-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Comparing two uniform distributions</a></span></li></ul></li></ul></li><li><span><a href=\"#Application-of-$f$-divergences-to-the-data-generated-from-two-methods-for-computing-the-Choquet-integral\" data-toc-modified-id=\"Application-of-$f$-divergences-to-the-data-generated-from-two-methods-for-computing-the-Choquet-integral-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Application of $f$-divergences to the data generated from two methods for computing the Choquet integral</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-the-Choquet-integral-of-normal-distributions\" data-toc-modified-id=\"For-the-Choquet-integral-of-normal-distributions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>For the Choquet integral of normal distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Presenting-the-data\" data-toc-modified-id=\"Presenting-the-data-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Presenting the data</a></span></li><li><span><a href=\"#Computing-the-$f$-divergences\" data-toc-modified-id=\"Computing-the-$f$-divergences-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Computing the $f$-divergences</a></span></li></ul></li><li><span><a href=\"#For-the-Choquet-integral-of-exponential-distributions\" data-toc-modified-id=\"For-the-Choquet-integral-of-exponential-distributions-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>For the Choquet integral of exponential distributions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Presenting-the-data\" data-toc-modified-id=\"Presenting-the-data-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Presenting the data</a></span></li><li><span><a href=\"#Computing-the-$f$-divergences\" data-toc-modified-id=\"Computing-the-$f$-divergences-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Computing the $f$-divergences</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Imports<span class=\"tocSkip\"></span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, uniform\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the size of the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence\n",
    "\n",
    "This section is inspired from [KL Divergence Python Example](https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our generic function\n",
    "\n",
    "We define our `KL_divergence` function using functions from `numpy`. We are careful with the result $\\lim\\limits_{x \\rightarrow 0} \\log(x) = -\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Kullback-Leibler divergence of two (empirical) probability distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : numpy.ndarray\n",
    "        Vector of the values of the first (discrete) probability distribution.\n",
    "    q : numpy.ndarray\n",
    "        Vector of the values of the second (discrete) probability distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : numpy.float64\n",
    "        Result of the computation of the Kullback-Leibler divergence of p from q.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = np.sum(np.where(p!=0, p*np.log(p/q), 0))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the KL divergence depend on the number of samples? Yes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_abs = np.arange(10, 20000, 50)\n",
    "l_KL = []\n",
    "for n in l_abs:\n",
    "    x = np.linspace(-10, 10, n)\n",
    "    p = norm.pdf(x, 0, 2)\n",
    "    q = norm.pdf(x, 1, 2)\n",
    "    l_KL.append(KL_divergence(p, q))\n",
    "\n",
    "plt.title('Influence of the number of samples considering $\\mathbb{P} = \\mathcal{N}(0, 2)$ and $\\mathbb{Q} = \\mathcal{N}(1, 2)$')\n",
    "plt.xlabel('Number of samples $n_p=n_q$')\n",
    "plt.ylabel('Kullback-Leibler divergence $D_{KL}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_abs, l_KL, 'o') ;\n",
    "plt.savefig('img/KL_n_1.png', dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should divide the KL divergence by the number of samples of one distribution in order to normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence_normalized(p, q):\n",
    "    \"\"\"\n",
    "    Kullback-Leibler divergence of two (empirical) probability distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : numpy.ndarray\n",
    "        Vector of the values of the first (discrete) probability distribution.\n",
    "    q : numpy.ndarray\n",
    "        Vector of the values of the second (discrete) probability distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : numpy.float64\n",
    "        Result of the computation of the Kullback-Leibler divergence of p from q.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = np.sum(np.where(p!=0, p*np.log(p/q), 0))\n",
    "    n = len(p)\n",
    "    \n",
    "    return res/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KL_divergence_normalized` does not depend on the number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_abs = np.arange(10, 20000, 50)\n",
    "l_KL_normalized = []\n",
    "for n in l_abs:\n",
    "    x = np.linspace(-10, 10, n)\n",
    "    p = norm.pdf(x, 0, 2)\n",
    "    q = norm.pdf(x, 1, 2)\n",
    "    l_KL_normalized.append(KL_divergence_normalized(p, q))\n",
    "\n",
    "plt.title('Influence of the number of samples considering $\\mathbb{P} = \\mathcal{N}(0, 2)$ and $\\mathbb{Q} = \\mathcal{N}(1, 2)$')\n",
    "plt.xlabel('Number of samples $n_p=n_q$')\n",
    "plt.ylabel('Normalized Kullback-Leibler divergence $D_{KL}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_abs, l_KL_normalized, 'o') ;\n",
    "plt.savefig('img/KL_n_2.png', dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running several simulations to interpret $D_{\\text{KL}}$\n",
    "\n",
    "### Comparing two normal distributions\n",
    "\n",
    "Here we consider two normal distributions $\\mathbb{P} = \\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathbb{Q} = \\mathcal{N}(\\mu_q, \\sigma_q)$.\n",
    "\n",
    "Now, we are going to plot two normal distributions using [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) and compute their KL divergence, for several values of mean and standard deviation. The goal is to see if the interpretation of the KL divergence corresponds to our intuition.\n",
    "\n",
    "We define a plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal_divergence(m_p, sd_p, m_q, sd_q, f_divergence):\n",
    "    \"\"\"\n",
    "    Plotting two normal distributions and computing their f-divergence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m_p : float\n",
    "        Mean of the normal distribution p.\n",
    "    sd_p : float\n",
    "        Standard deviation of the normal distribution p.\n",
    "    m_q : float\n",
    "        Mean of the normal distribution p.\n",
    "    sd_q : float\n",
    "        Standard deviation distribution p.\n",
    "    f_divergence : {KL_divergence, hellinger_distance, varational_distance}\n",
    "        Function that computes the f-divergence we choose.\n",
    "        \n",
    "    Plots\n",
    "    -------\n",
    "    Plots (on the same figure) the two normal distributions and their f-divergence in the title.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "    p = norm.pdf(x, m_p, sd_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "    q = norm.pdf(x, m_q, sd_q) # second normal distribution of mean m_q and standard deviation sd_q\n",
    "    \n",
    "    plt.title('The %s of $p$ from $q$ is %1.3f \\n (with $p$ and $q$ normal distributions)'\n",
    "              % (f_divergence.__name__, f_divergence(p, q)))\n",
    "    plt.plot(x, p)\n",
    "    plt.plot(x, q, c='red')\n",
    "    txt1 = '$\\mu_p = %1.1f$ and $\\sigma_p = %1.0f$' % (m_p, sd_p)\n",
    "    txt2 = '$\\mu_q = %1.1f$ and $\\sigma_q = %1.0f$' % (m_q, sd_q)\n",
    "    plt.legend([txt1, txt2])\n",
    "    #plt.savefig('img/KL_plot.png', dpi=120) # to save the figure\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_divergence(0, 2, 1, 2, KL_divergence_normalized)\n",
    "plot_normal_divergence(0, 2, 5, 2, KL_divergence_normalized)\n",
    "plot_normal_divergence(5, 2, 0, 2, KL_divergence_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the lower the KL divergence, the closer the two distributions are to one another.\n",
    "\n",
    "It is important to note that the KL divergence is not symmetrical. In other words, if we switch `p` for `q` and vice versa, we get a different result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Influence of the difference of means $\\mu_q-\\mu_p$ on $D_\\text{KL}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_KL_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "\n",
    "mu_p = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = norm.pdf(x, mu_p, sigma_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20+0.01, 0.01)\n",
    "for mu_q in l_abs:\n",
    "    q = norm.pdf(x, mu_q, sigma_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_KL_mu.append([KL_divergence_normalized(p, q)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (mu_p, sigma_p)\n",
    "txt = txt + '$\\sigma_q = %1.0f$' % (sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical normalized Kullback-Leibler divergence $D_{nKL}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_mu, l_KL_mu, 'o') ;\n",
    "plt.savefig('img/KL_diff_mu.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_mu), 2)\n",
    "y = np.asarray(l_KL_mu)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', reg.score(X, y))\n",
    "print('The regression coefficients are: \\n', reg.coef_)\n",
    "print('The regression intercept is: \\n', reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_KL_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "\n",
    "mu_p = 3\n",
    "sigma_p = 4\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = norm.pdf(x, mu_p, sigma_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "sigma_q = 4\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20+0.01, 0.01)\n",
    "for mu_q in l_abs:\n",
    "    q = norm.pdf(x, mu_q, sigma_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_KL_mu.append([KL_divergence_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (mu_p, sigma_p)\n",
    "txt = txt + '$\\sigma_q = %1.0f$' % (sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical normalized Kullback-Leibler divergence $D_{nKL}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_mu, l_KL_mu, 'o') ;\n",
    "plt.savefig('img/KL_diff_mu_2.png', dpi=120) # to save the figure\n",
    "\n",
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_mu), 2)\n",
    "y = np.asarray(l_KL_mu)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', reg.score(X, y))\n",
    "print('The regression coefficients are: \\n', reg.coef_)\n",
    "print('The regression intercept is: \\n', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $D_\\text{KL}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_KL_sigma = []\n",
    "l_diff_sigma = []\n",
    "\n",
    "x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "\n",
    "mu_p = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = norm.pdf(x, mu_p, sigma_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "mu_q = 0\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(2, 40, 0.01)\n",
    "for sigma_q in l_abs:\n",
    "    q = norm.pdf(x, mu_q, sigma_q)\n",
    "    l_diff_sigma.append([sigma_q-sigma_p])\n",
    "    l_KL_sigma.append([KL_divergence_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (mu_p, sigma_p)\n",
    "txt = txt + '$\\mu_q = %1.0f$' % (mu_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of standard deviations $\\sigma_q-\\sigma_p$')\n",
    "plt.ylabel('Empirical normalized Kullback-Leibler divergence $D_{nKL}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_sigma, l_KL_sigma, 'o') ;\n",
    "plt.savefig('img/KL_diff_sigma.png', dpi=120) # to save the figure\n",
    "\n",
    "reg = LinearRegression()\n",
    "X = np.power(np.asarray(l_diff_sigma), 1/2)\n",
    "y = np.asarray(l_KL_sigma)\n",
    "\n",
    "reg.fit(X, y)\n",
    "print('The regression score is: \\n', reg.score(X, y))\n",
    "print('The regression coefficients are: \\n', reg.coef_)\n",
    "print('The regression intercept is: \\n', reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of two exponential distributions\n",
    "\n",
    "Here we consider two exponential distributions $\\mathbb{P} = \\mathcal{E}(\\lambda_p)$ and $\\mathbb{Q} = \\mathcal{E}(\\lambda_q)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_KL_lambda = []\n",
    "l_diff_lambda = []\n",
    "\n",
    "lambda_p = 1\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1)\n",
    "Xp = np.random.exponential(lambda_p, n_p)\n",
    "p, bin_edges = np.histogram(Xp, density=True)\n",
    "\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(2, 20, 0.01)\n",
    "for lambda_q in l_abs:\n",
    "    np.random.seed(int(lambda_q*1000)) # random seed for reproducability, different each time\n",
    "    Xq = np.random.exponential(lambda_q, n_q)\n",
    "    q, bin_edges = np.histogram(Xq, density=True)\n",
    "    l_diff_lambda.append([lambda_q-lambda_p])\n",
    "    l_KL_lambda.append([KL_divergence_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two exponential distributions $\\mathcal{E}(\\lambda_p)$ and $\\mathcal{E}(\\lambda_q)$ \\n'\n",
    "txt = txt + 'with $\\lambda_p = %1.0f$' % (lambda_p)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of parameters $\\lambda_q-\\lambda_p$')\n",
    "plt.ylabel('Empirical normalized Kullback-Leibler divergence $D_{nKL}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_lambda, l_KL_lambda, 'o') ;\n",
    "plt.savefig('img/KL_diff_lambda.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of two uniform distributions\n",
    "\n",
    "Here we consider two normal distributions $\\mathbb{P} = \\mathbb{U}\\left([a, a+h]\\right)$ and $\\mathbb{Q} = \\mathbb{U}\\left([r, r+h]\\right)$ where $h$ is the length of the intervals.\n",
    "\n",
    "Note: $a$ is called the **interval start** of $[a, a+h]$.\n",
    "\n",
    "We use [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html)'s function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_KL_unif = []\n",
    "l_diff_unif = []\n",
    "\n",
    "x = np.arange(1, 20, 0.001) # x-axis of our plot\n",
    "\n",
    "h = 2\n",
    "a = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = uniform.pdf(x, a, a+h) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20, 0.01)\n",
    "for r in l_abs:\n",
    "    q = uniform.pdf(x, r, r+h)\n",
    "    l_diff_unif.append([r-a])\n",
    "    l_KL_unif.append([KL_divergence_normalized(p, q)]) # +1 so the values of p and q are strictly positive\n",
    "\n",
    "txt = 'Comparison of two uniform distributions $\\mathbb{U}([a, a+h])$ and $\\mathbb{U}([r, r+h])$ \\n'\n",
    "txt = txt + 'with $a = %1.0f$, $h = %1.0f$' % (a, h)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of interval starts $r-a$')\n",
    "plt.ylabel('Empirical normalized Kullback-Leibler divergence $D_{nKL}(\\mathbb{P} || \\mathbb{Q})$')\n",
    "plt.plot(l_diff_unif, l_KL_unif, 'o') ;\n",
    "plt.savefig('img/KL_diff_unif.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hellinger distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our generic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger_distance_normalized(p, q):\n",
    "    res = np.sum((np.sqrt(p)-np.sqrt(q))**2)\n",
    "    n = len(p)\n",
    "    return res/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_abs = np.arange(10, 20000, 50)\n",
    "l_H = []\n",
    "for n in l_abs:\n",
    "    x = np.linspace(-10, 10, n)\n",
    "    p = norm.pdf(x, 0, 2)\n",
    "    q = norm.pdf(x, 1, 2)\n",
    "    l_H.append(hellinger_distance_normalized(p, q))\n",
    "\n",
    "plt.title('Influence of the number of samples considering $\\mathbb{P} = \\mathcal{N}(0, 2)$ and $\\mathbb{Q} = \\mathcal{N}(1, 2)$')\n",
    "plt.xlabel('Number of samples $n_p=n_q$')\n",
    "plt.ylabel('Normalized Hellinger distance $D_{H}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_abs, l_H, 'o') ;\n",
    "plt.savefig('img/H_n_1.png', dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_divergence(0, 2, 1, 2, hellinger_distance_normalized)\n",
    "plot_normal_divergence(0, 2, 2, 2, hellinger_distance_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running several simulations to interpret $D_H$\n",
    "\n",
    "### Comparing two normal distributions\n",
    "\n",
    "Here we consider two normal distributions $\\mathbb{P} = \\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathbb{Q} = \\mathcal{N}(\\mu_q, \\sigma_q)$.\n",
    "\n",
    "#### Influence of the difference of means $\\mu_q-\\mu_p$ on $D_H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_HD_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "\n",
    "mu_p = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = norm.pdf(x, mu_p, sigma_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20, 0.01)\n",
    "for mu_q in l_abs:\n",
    "    q = norm.pdf(x, mu_q, sigma_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_HD_mu.append([hellinger_distance_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (mu_p, sigma_p)\n",
    "txt = txt + '$\\sigma_q = %1.0f$' % (sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical normalized Hellinger distance $D_{nH}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_mu, l_HD_mu, 'o') ;\n",
    "plt.savefig('img/HD_diff_mu.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $D_H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_HD_sigma = []\n",
    "l_diff_sigma = []\n",
    "\n",
    "x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "\n",
    "mu_p = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = norm.pdf(x, mu_p, sigma_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "mu_q = 0\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(2, 40, 0.01)\n",
    "for sigma_q in l_abs:\n",
    "    q = norm.pdf(x, mu_q, sigma_q)\n",
    "    l_diff_sigma.append([sigma_q-sigma_p])\n",
    "    l_HD_sigma.append([hellinger_distance_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (mu_p, sigma_p)\n",
    "txt = txt + '$\\mu_q = %1.0f$' % (mu_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of standard deviations $\\sigma_q-\\sigma_p$')\n",
    "plt.ylabel('Normalized empirical Hellinger distance $D_{nH}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_sigma, l_HD_sigma, 'o') ;\n",
    "plt.savefig('img/HD_diff_sigma.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing two exponential distributions\n",
    "\n",
    "Here we consider two exponential distributions $\\mathbb{P} = \\mathcal{E}(\\lambda_p)$ and $\\mathbb{Q} = \\mathcal{E}(\\lambda_q)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_HD_lambda = []\n",
    "l_diff_lambda = []\n",
    "\n",
    "lambda_p = 1\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1)\n",
    "Xp = np.random.exponential(lambda_p, n_p)\n",
    "p, bin_edges = np.histogram(Xp, density=True)\n",
    "\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(2, 20, 0.01)\n",
    "for lambda_q in l_abs:\n",
    "    np.random.seed(int(lambda_q*1000)) # random seed for reproducability, different each time\n",
    "    Xq = np.random.exponential(lambda_q, n_q)\n",
    "    q, bin_edges = np.histogram(Xq, density=True)\n",
    "    l_diff_lambda.append([lambda_q-lambda_p])\n",
    "    l_HD_lambda.append([hellinger_distance_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two exponential distributions $\\mathcal{E}(\\lambda_p)$ and $\\mathcal{E}(\\lambda_q)$ \\n'\n",
    "txt = txt + 'with $\\lambda_p = %1.0f$' % (lambda_p)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of parameters $\\lambda_q-\\lambda_p$')\n",
    "plt.ylabel('Normalized empirical Hellinger distance $D_{nH}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_lambda, l_HD_lambda, 'o') ;\n",
    "plt.savefig('img/HD_diff_lambda.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing two uniform distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_HD_unif = []\n",
    "l_diff_unif = []\n",
    "\n",
    "x = np.arange(1, 20, 0.001) # x-axis of our plot\n",
    "\n",
    "h = 2\n",
    "a = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = uniform.pdf(x, a, a+h) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20, 0.01)\n",
    "for r in l_abs:\n",
    "    q = uniform.pdf(x, r, r+h)\n",
    "    l_diff_unif.append([r-a])\n",
    "    l_HD_unif.append([hellinger_distance_normalized(p, q)]) # +1 so the values of p and q are strictly positive\n",
    "\n",
    "txt = 'Comparison of two uniform distributions $\\mathbb{U}([a, a+h])$ and $\\mathbb{U}([r, r+h])$ \\n'\n",
    "txt = txt + 'with $a = %1.0f$, $h = %1.0f$' % (a, h)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of interval starts $r-a$')\n",
    "plt.ylabel('Empirical Hellinger distance $D_{nH}(\\mathbb{P} || \\mathbb{Q})$')\n",
    "plt.plot(l_diff_unif, l_HD_unif, 'o') ;\n",
    "plt.savefig('img/HD_diff_unif.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our generic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_distance_normalized(p, q):\n",
    "    res = np.sum(np.abs(p-q))\n",
    "    n = len(p)\n",
    "    return res/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_abs = np.arange(10, 20000, 50)\n",
    "l_V = []\n",
    "for n in l_abs:\n",
    "    x = np.linspace(-10, 10, n)\n",
    "    p = norm.pdf(x, 0, 2)\n",
    "    q = norm.pdf(x, 1, 2)\n",
    "    l_V.append(variational_distance_normalized(p, q))\n",
    "\n",
    "plt.title('Influence of the number of samples considering $\\mathbb{P} = \\mathcal{N}(0, 2)$ and $\\mathbb{Q} = \\mathcal{N}(1, 2)$')\n",
    "plt.xlabel('Number of samples $n_p=n_q$')\n",
    "plt.ylabel('Normalized Variational distance $D_{H}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_abs, l_V, 'o') ;\n",
    "plt.savefig('img/V_n_1.png', dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_divergence(0, 2, 1, 2, variational_distance_normalized)\n",
    "plot_normal_divergence(0, 2, 2, 2, variational_distance_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running several simulations to interpret $D_V$\n",
    "\n",
    "### Comparing two normal distributions\n",
    "\n",
    "Here we consider two normal distributions $\\mathbb{P} = \\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathbb{Q} = \\mathcal{N}(\\mu_q, \\sigma_q)$.\n",
    "\n",
    "#### Influence of the difference of means $\\mu_q-\\mu_p$ on $D_V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_VD_mu = []\n",
    "l_diff_mu = []\n",
    "\n",
    "x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "\n",
    "mu_p = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = norm.pdf(x, mu_p, sigma_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20, 0.01)\n",
    "for mu_q in l_abs:\n",
    "    q = norm.pdf(x, mu_q, sigma_q)\n",
    "    l_diff_mu.append([mu_q-mu_p])\n",
    "    l_VD_mu.append([variational_distance_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (mu_p, sigma_p)\n",
    "txt = txt + '$\\sigma_q = %1.0f$' % (sigma_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of means $\\mu_q-\\mu_p$')\n",
    "plt.ylabel('Empirical normalized Variational distance $D_{nV}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_mu, l_VD_mu, 'o') ;\n",
    "plt.savefig('img/VD_diff_mu.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Influence of the difference of standard deviations $\\sigma_q-\\sigma_p$ on $D_V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_VD_sigma = []\n",
    "l_diff_sigma = []\n",
    "\n",
    "x = np.arange(-10, 10, 0.001) # x-axis of our plot\n",
    "\n",
    "mu_p = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = norm.pdf(x, mu_p, sigma_p) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "mu_q = 0\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(2, 40, 0.01)\n",
    "for sigma_q in l_abs:\n",
    "    q = norm.pdf(x, mu_q, sigma_q)\n",
    "    l_diff_sigma.append([sigma_q-sigma_p])\n",
    "    l_VD_sigma.append([variational_distance_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two normal distributions $\\mathcal{N}(\\mu_p, \\sigma_p)$ and $\\mathcal{N}(\\mu_q, \\sigma_q)$ \\n'\n",
    "txt = txt + 'with $\\mu_p = %1.0f$, $\\sigma_p = %1.0f, $' % (mu_p, sigma_p)\n",
    "txt = txt + '$\\mu_q = %1.0f$' % (mu_q)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of standard deviations $\\sigma_q-\\sigma_p$')\n",
    "plt.ylabel('Normalized empirical Hellinger distance $D_{nH}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_sigma, l_VD_sigma, 'o') ;\n",
    "plt.savefig('img/VD_diff_sigma.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing two exponential distributions\n",
    "\n",
    "Here we consider two exponential distributions $\\mathbb{P} = \\mathcal{E}(\\lambda_p)$ and $\\mathbb{Q} = \\mathcal{E}(\\lambda_q)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_VD_lambda = []\n",
    "l_diff_lambda = []\n",
    "\n",
    "lambda_p = 1\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "np.random.seed(1)\n",
    "Xp = np.random.exponential(lambda_p, n_p)\n",
    "p, bin_edges = np.histogram(Xp, density=True)\n",
    "\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(2, 20, 0.01)\n",
    "for lambda_q in l_abs:\n",
    "    np.random.seed(int(lambda_q*1000)) # random seed for reproducability, different each time\n",
    "    Xq = np.random.exponential(lambda_q, n_q)\n",
    "    q, bin_edges = np.histogram(Xq, density=True)\n",
    "    l_diff_lambda.append([lambda_q-lambda_p])\n",
    "    l_VD_lambda.append([variational_distance_normalized(p, q)])\n",
    "\n",
    "txt = 'Comparison of two exponential distributions $\\mathcal{E}(\\lambda_p)$ and $\\mathcal{E}(\\lambda_q)$ \\n'\n",
    "txt = txt + 'with $\\lambda_p = %1.0f$' % (lambda_p)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of parameters $\\lambda_q-\\lambda_p$')\n",
    "plt.ylabel('Empirical normalized Variational distance $D_{nV}(\\mathbb{P}, \\mathbb{Q})$')\n",
    "plt.plot(l_diff_lambda, l_HD_lambda, 'o') ;\n",
    "plt.savefig('img/VD_diff_lambda.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing two uniform distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_VD_unif = []\n",
    "l_diff_unif = []\n",
    "\n",
    "x = np.arange(1, 20, 0.001) # x-axis of our plot\n",
    "\n",
    "h = 2\n",
    "a = 0\n",
    "sigma_p = 2\n",
    "n_p = 30 # after several simulations, 30 seems enough\n",
    "p = uniform.pdf(x, a, a+h) # first normal distribution of mean m_p and standard deviation sd_p\n",
    "\n",
    "sigma_q = 2\n",
    "n_q = n_p\n",
    "\n",
    "l_abs = np.arange(0, 20, 0.01)\n",
    "for r in l_abs:\n",
    "    q = uniform.pdf(x, r, r+h)\n",
    "    l_diff_unif.append([r-a])\n",
    "    l_VD_unif.append([variational_distance_normalized(p, q)]) # +1 so the values of p and q are strictly positive\n",
    "\n",
    "txt = 'Comparison of two uniform distributions $\\mathbb{U}([a, a+h])$ and $\\mathbb{U}([r, r+h])$ \\n'\n",
    "txt = txt + 'with $a = %1.0f$, $h = %1.0f$' % (a, h)\n",
    "plt.title(txt)\n",
    "plt.xlabel('Difference of interval starts $r-a$')\n",
    "plt.ylabel('Empirical Variational distance $D_{nV}(\\mathbb{P} || \\mathbb{Q})$')\n",
    "plt.plot(l_diff_unif, l_VD_unif, 'o') ;\n",
    "plt.savefig('img/VD_diff_unif.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of $f$-divergences to the data generated from two methods for computing the Choquet integral\n",
    "\n",
    "## For the Choquet integral of normal distributions\n",
    "\n",
    "### Presenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_df = pd.read_csv('data/P1_normal.csv', sep=',', header=None)\n",
    "P2_df = pd.read_csv('data/P2_normal.csv', sep=',', header=None)\n",
    "\n",
    "P1_array = P1_df.values[0]\n",
    "P2_array = P2_df.values[0]\n",
    "\n",
    "P1_list = P1_df.values.tolist()[0]\n",
    "P2_list = P2_df.values.tolist()[0]\n",
    "\n",
    "print(len(P1_list))\n",
    "print(len(P2_list))\n",
    "\n",
    "plt.title('Plotting the distributions from two Choquet integrals \\n of normal distributions') ;\n",
    "plt.plot(P1_list, 'o') ;\n",
    "plt.plot(P2_list, 'o') ;\n",
    "plt.legend(['with the direct method', 'with the new formula']) ;\n",
    "plt.savefig('img/choquet_normal_P.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the $f$-divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL_divergence_normalized(P1_array, P2_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hellinger_distance_normalized(P1_array, P2_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_distance_normalized(P1_array, P2_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the 0 values change the computation ? Yes, because of the normalization. Indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.array([0, 0, 0, 0, 10, 20, 30, 40, 50, 60, 70, 70, 80])\n",
    "test2 = np.array([0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8])\n",
    "KL_divergence_normalized(test1, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.array([10, 20, 30, 40, 50, 60, 70, 70, 80])\n",
    "test2 = np.array([1, 2, 3, 4, 5, 6, 7, 7, 8])\n",
    "KL_divergence_normalized(test1, test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the Choquet integral of exponential distributions\n",
    "\n",
    "### Presenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_df = pd.read_csv('data/P1_exp.csv', sep=',', header=None)\n",
    "P2_df = pd.read_csv('data/P2_exp.csv', sep=',', header=None)\n",
    "\n",
    "P1_array = P1_df.values[0]\n",
    "P2_array = P2_df.values[0]\n",
    "\n",
    "P1_list = P1_df.values.tolist()[0]\n",
    "P2_list = P2_df.values.tolist()[0]\n",
    "\n",
    "print(len(P1_list))\n",
    "print(len(P2_list))\n",
    "\n",
    "plt.title('Plotting the distributions from two Choquet integrals \\n of normal distributions') ;\n",
    "plt.plot(P1_list, 'o') ;\n",
    "plt.plot(P2_list, 'o') ;\n",
    "plt.legend(['with the direct method', 'with the new formula']) ;\n",
    "plt.savefig('img/choquet_exp_P.png', dpi=120) # to save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the $f$-divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL_divergence_normalized(P1_array, P2_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hellinger_distance_normalized(P1_array, P2_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_distance_normalized(P1_array, P2_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
